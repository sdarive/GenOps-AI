#!/usr/bin/env python3
"""
GenOps Helicone AI Gateway Pricing Calculator

This module provides comprehensive cost calculations for Helicone AI gateway
operations, including both provider costs and gateway service fees. It supports
all major AI providers routed through Helicone with real-time pricing data.

Features:
- Accurate provider-specific cost calculations
- Helicone gateway service fee calculations  
- Cross-provider cost comparison and optimization
- Multi-model pricing with latest rates (November 2024)
- Enterprise pricing tiers and volume discounts
- Cost forecasting and budgeting utilities
- Regional pricing variations and currency support

Supported Providers:
- OpenAI (GPT-4, GPT-3.5, Embeddings, etc.)
- Anthropic (Claude 3 family, Legacy models)
- Google (Vertex AI, Gemini Pro/Flash)
- Groq (Llama, Mistral, Gemma models)
- Together AI (Open source models)
- Cohere (Command, Embed models)
- Hugging Face (Inference Endpoints)
"""

import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

logger = logging.getLogger(__name__)

class PricingTier(Enum):
    """Helicone pricing tiers."""
    FREE = "free"           # Free tier: 100k requests/month
    GROWTH = "growth"       # Growth tier: Higher limits
    PRO = "pro"            # Pro tier: Advanced features
    ENTERPRISE = "enterprise"  # Enterprise: Custom pricing

@dataclass
class ModelPricing:
    """Pricing information for a specific model."""
    provider: str
    model: str
    input_price_per_1k: float
    output_price_per_1k: float
    currency: str = "USD"
    effective_date: str = "2024-11-01"
    notes: Optional[str] = None

@dataclass
class GatewayPricingTier:
    """Helicone gateway pricing tier information."""
    tier: PricingTier
    monthly_requests_included: int
    overage_price_per_1k: float
    features: List[str] = field(default_factory=list)
    monthly_fee: float = 0.0

class HeliconePricingCalculator:
    """
    Comprehensive pricing calculator for Helicone AI gateway operations.
    
    Calculates accurate costs including both AI provider charges and 
    Helicone gateway service fees across all supported providers.
    """
    
    def __init__(self):
        """Initialize pricing calculator with current rates."""
        self.pricing_data = self._load_pricing_data()
        self.gateway_tiers = self._load_gateway_pricing()
        self.currency = "USD"
        
    def _load_pricing_data(self) -> Dict[str, Dict[str, ModelPricing]]:
        """Load current pricing data for all supported providers."""
        # OpenAI Pricing (November 2024)
        openai_models = {
            # GPT-4 Turbo family
            "gpt-4-turbo-preview": ModelPricing("openai", "gpt-4-turbo-preview", 10.00, 30.00),
            "gpt-4-turbo": ModelPricing("openai", "gpt-4-turbo", 10.00, 30.00),
            "gpt-4-0125-preview": ModelPricing("openai", "gpt-4-0125-preview", 10.00, 30.00),
            
            # GPT-4 Standard
            "gpt-4": ModelPricing("openai", "gpt-4", 30.00, 60.00),
            "gpt-4-0613": ModelPricing("openai", "gpt-4-0613", 30.00, 60.00),
            "gpt-4-32k": ModelPricing("openai", "gpt-4-32k", 60.00, 120.00),
            
            # GPT-3.5 family
            "gpt-3.5-turbo": ModelPricing("openai", "gpt-3.5-turbo", 0.50, 1.50),
            "gpt-3.5-turbo-0125": ModelPricing("openai", "gpt-3.5-turbo-0125", 0.50, 1.50),
            "gpt-3.5-turbo-instruct": ModelPricing("openai", "gpt-3.5-turbo-instruct", 1.50, 2.00),
            
            # Embeddings
            "text-embedding-ada-002": ModelPricing("openai", "text-embedding-ada-002", 0.10, 0.00),
            "text-embedding-3-small": ModelPricing("openai", "text-embedding-3-small", 0.02, 0.00),
            "text-embedding-3-large": ModelPricing("openai", "text-embedding-3-large", 0.13, 0.00),
            
            # Audio models
            "whisper-1": ModelPricing("openai", "whisper-1", 6.00, 0.00, notes="Per minute of audio"),
            "tts-1": ModelPricing("openai", "tts-1", 15.00, 0.00, notes="Per 1K characters"),
            "tts-1-hd": ModelPricing("openai", "tts-1-hd", 30.00, 0.00, notes="Per 1K characters")
        }
        
        # Anthropic Pricing (November 2024)
        anthropic_models = {
            # Claude 3 family
            "claude-3-5-sonnet-20241022": ModelPricing("anthropic", "claude-3-5-sonnet-20241022", 3.00, 15.00),
            "claude-3-opus-20240229": ModelPricing("anthropic", "claude-3-opus-20240229", 15.00, 75.00),
            "claude-3-sonnet-20240229": ModelPricing("anthropic", "claude-3-sonnet-20240229", 3.00, 15.00),
            "claude-3-haiku-20240307": ModelPricing("anthropic", "claude-3-haiku-20240307", 0.25, 1.25),
            
            # Legacy Claude models
            "claude-2.1": ModelPricing("anthropic", "claude-2.1", 8.00, 24.00),
            "claude-2.0": ModelPricing("anthropic", "claude-2.0", 8.00, 24.00),
            "claude-instant-1.2": ModelPricing("anthropic", "claude-instant-1.2", 0.80, 2.40)
        }
        
        # Google Vertex AI Pricing (November 2024)
        google_models = {
            # Gemini Pro family
            "gemini-pro": ModelPricing("vertex", "gemini-pro", 0.50, 1.50),
            "gemini-pro-vision": ModelPricing("vertex", "gemini-pro-vision", 0.50, 1.50),
            "gemini-1.5-pro": ModelPricing("vertex", "gemini-1.5-pro", 7.00, 21.00),
            "gemini-1.5-flash": ModelPricing("vertex", "gemini-1.5-flash", 0.075, 0.30),
            
            # Text models
            "text-bison": ModelPricing("vertex", "text-bison", 1.00, 1.00),
            "text-bison-32k": ModelPricing("vertex", "text-bison-32k", 1.25, 1.25),
            "chat-bison": ModelPricing("vertex", "chat-bison", 1.00, 1.00),
            "chat-bison-32k": ModelPricing("vertex", "chat-bison-32k", 1.25, 1.25),
            
            # Embeddings
            "textembedding-gecko": ModelPricing("vertex", "textembedding-gecko", 0.10, 0.00)
        }
        
        # Groq Pricing (November 2024)
        groq_models = {
            # Llama models
            "llama2-70b-4096": ModelPricing("groq", "llama2-70b-4096", 0.70, 0.80),
            "llama3-8b-8192": ModelPricing("groq", "llama3-8b-8192", 0.05, 0.08),
            "llama3-70b-8192": ModelPricing("groq", "llama3-70b-8192", 0.59, 0.79),
            
            # Mixtral models
            "mixtral-8x7b-32768": ModelPricing("groq", "mixtral-8x7b-32768", 0.24, 0.24),
            
            # Gemma models
            "gemma-7b-it": ModelPricing("groq", "gemma-7b-it", 0.07, 0.07)
        }
        
        # Together AI Pricing (November 2024)
        together_models = {
            # Meta Llama models
            "meta-llama/Llama-2-7b-chat-hf": ModelPricing("together", "meta-llama/Llama-2-7b-chat-hf", 0.20, 0.20),
            "meta-llama/Llama-2-13b-chat-hf": ModelPricing("together", "meta-llama/Llama-2-13b-chat-hf", 0.25, 0.25),
            "meta-llama/Llama-2-70b-chat-hf": ModelPricing("together", "meta-llama/Llama-2-70b-chat-hf", 0.90, 0.90),
            
            # Mistral models
            "mistralai/Mistral-7B-Instruct-v0.1": ModelPricing("together", "mistralai/Mistral-7B-Instruct-v0.1", 0.20, 0.20),
            "mistralai/Mixtral-8x7B-Instruct-v0.1": ModelPricing("together", "mistralai/Mixtral-8x7B-Instruct-v0.1", 0.60, 0.60)
        }
        
        # Cohere Pricing (November 2024)
        cohere_models = {
            # Command models
            "command": ModelPricing("cohere", "command", 15.00, 15.00),
            "command-light": ModelPricing("cohere", "command-light", 0.30, 0.60),
            "command-nightly": ModelPricing("cohere", "command-nightly", 15.00, 15.00),
            
            # Embed models
            "embed-english-v3.0": ModelPricing("cohere", "embed-english-v3.0", 0.10, 0.00),
            "embed-multilingual-v3.0": ModelPricing("cohere", "embed-multilingual-v3.0", 0.10, 0.00)
        }
        
        return {
            "openai": openai_models,
            "anthropic": anthropic_models,
            "vertex": google_models,
            "groq": groq_models, 
            "together": together_models,
            "cohere": cohere_models
        }
    
    def _load_gateway_pricing(self) -> Dict[PricingTier, GatewayPricingTier]:
        """Load Helicone gateway pricing tiers."""
        return {
            PricingTier.FREE: GatewayPricingTier(
                tier=PricingTier.FREE,
                monthly_requests_included=100000,  # 100k requests free
                overage_price_per_1k=0.05,        # $0.05 per 1k requests over limit
                monthly_fee=0.0,
                features=[
                    "Request logging and analytics",
                    "Basic dashboard",
                    "Community support",
                    "100k requests/month included"
                ]
            ),
            PricingTier.GROWTH: GatewayPricingTier(
                tier=PricingTier.GROWTH,
                monthly_requests_included=1000000,  # 1M requests
                overage_price_per_1k=0.03,         # Lower overage rate
                monthly_fee=20.0,
                features=[
                    "Advanced analytics",
                    "Custom properties",
                    "Webhook integrations",
                    "Priority support",
                    "1M requests/month included"
                ]
            ),
            PricingTier.PRO: GatewayPricingTier(
                tier=PricingTier.PRO,
                monthly_requests_included=10000000,  # 10M requests
                overage_price_per_1k=0.02,          # Even lower overage
                monthly_fee=100.0,
                features=[
                    "Advanced filtering",
                    "Custom dashboards", 
                    "API access",
                    "SSO integration",
                    "10M requests/month included"
                ]
            ),
            PricingTier.ENTERPRISE: GatewayPricingTier(
                tier=PricingTier.ENTERPRISE,
                monthly_requests_included=float('inf'),  # Unlimited
                overage_price_per_1k=0.0,               # Custom pricing
                monthly_fee=0.0,                         # Custom pricing
                features=[
                    "Unlimited requests",
                    "Self-hosted deployment",
                    "Custom integrations",
                    "Dedicated support",
                    "Custom SLA"
                ]
            )
        }
    
    def calculate_provider_cost(
        self, 
        provider: str, 
        model: str, 
        input_tokens: int, 
        output_tokens: int = 0
    ) -> Tuple[float, float, float]:
        """
        Calculate provider-specific costs.
        
        Args:
            provider: AI provider (openai, anthropic, etc.)
            model: Model name
            input_tokens: Number of input tokens
            output_tokens: Number of output tokens
            
        Returns:
            (input_cost, output_cost, total_cost)
        """
        provider_models = self.pricing_data.get(provider.lower(), {})
        model_pricing = provider_models.get(model)
        
        if not model_pricing:
            logger.warning(f"Pricing not found for {provider}/{model}, using default rates")
            # Use default pricing based on provider
            default_rates = self._get_default_rates(provider)
            input_cost = (input_tokens / 1000) * default_rates["input"]
            output_cost = (output_tokens / 1000) * default_rates["output"]
            return input_cost, output_cost, input_cost + output_cost
        
        # Calculate costs based on model pricing
        input_cost = (input_tokens / 1000) * model_pricing.input_price_per_1k
        output_cost = (output_tokens / 1000) * model_pricing.output_price_per_1k
        total_cost = input_cost + output_cost
        
        return input_cost, output_cost, total_cost
    
    def calculate_helicone_cost(
        self, 
        requests_this_month: int,
        tier: PricingTier = PricingTier.FREE
    ) -> Tuple[float, float]:
        """
        Calculate Helicone gateway service costs.
        
        Args:
            requests_this_month: Number of requests made this month
            tier: Helicone pricing tier
            
        Returns:
            (monthly_fee, overage_cost)
        """
        tier_pricing = self.gateway_tiers[tier]
        
        monthly_fee = tier_pricing.monthly_fee
        
        if requests_this_month <= tier_pricing.monthly_requests_included:
            overage_cost = 0.0
        else:
            overage_requests = requests_this_month - tier_pricing.monthly_requests_included
            overage_cost = (overage_requests / 1000) * tier_pricing.overage_price_per_1k
        
        return monthly_fee, overage_cost
    
    def calculate_gateway_cost(
        self,
        provider: str,
        model: str,
        input_tokens: int,
        output_tokens: int,
        request_time: float,
        requests_this_month: int = 50000,
        tier: PricingTier = PricingTier.FREE
    ) -> Tuple[float, float, float]:
        """
        Calculate total gateway cost including provider and Helicone fees.
        
        Args:
            provider: AI provider
            model: Model name
            input_tokens: Input tokens
            output_tokens: Output tokens
            request_time: Request processing time
            requests_this_month: Total requests this month (for tier calculation)
            tier: Helicone pricing tier
            
        Returns:
            (provider_cost, helicone_cost, total_cost)
        """
        # Calculate provider cost
        input_cost, output_cost, provider_cost = self.calculate_provider_cost(
            provider, model, input_tokens, output_tokens
        )
        
        # Calculate Helicone service cost (prorated per request)
        monthly_fee, overage_cost = self.calculate_helicone_cost(requests_this_month, tier)
        
        # Prorate monthly costs per request
        monthly_requests_estimate = max(requests_this_month, 1000)  # Minimum 1k for calculation
        helicone_cost_per_request = (monthly_fee + overage_cost) / monthly_requests_estimate
        
        total_cost = provider_cost + helicone_cost_per_request
        
        return provider_cost, helicone_cost_per_request, total_cost
    
    def estimate_request_cost(
        self, 
        provider: str, 
        model: str,
        estimated_input_tokens: int = 100,
        estimated_output_tokens: int = 50
    ) -> float:
        """
        Quick cost estimation for routing decisions.
        
        Args:
            provider: AI provider
            model: Model name
            estimated_input_tokens: Estimated input tokens
            estimated_output_tokens: Estimated output tokens
            
        Returns:
            Estimated total cost
        """
        _, _, provider_cost = self.calculate_provider_cost(
            provider, model, estimated_input_tokens, estimated_output_tokens
        )
        
        # Add approximate gateway overhead (minimal for routing decisions)
        gateway_overhead = 0.0001  # $0.0001 per request approximation
        
        return provider_cost + gateway_overhead
    
    def compare_provider_costs(
        self,
        providers: List[str],
        model_preferences: Dict[str, str],
        input_tokens: int,
        output_tokens: int
    ) -> Dict[str, Dict[str, float]]:
        """
        Compare costs across multiple providers.
        
        Args:
            providers: List of providers to compare
            model_preferences: Provider to model mapping
            input_tokens: Input tokens
            output_tokens: Output tokens
            
        Returns:
            Dictionary with cost comparison data
        """
        comparison = {}
        
        for provider in providers:
            model = model_preferences.get(provider, "default")
            
            try:
                provider_cost, helicone_cost, total_cost = self.calculate_gateway_cost(
                    provider, model, input_tokens, output_tokens, 1.0
                )
                
                comparison[provider] = {
                    "model": model,
                    "provider_cost": provider_cost,
                    "helicone_cost": helicone_cost,
                    "total_cost": total_cost,
                    "cost_per_token": total_cost / max(input_tokens + output_tokens, 1)
                }
            except Exception as e:
                logger.warning(f"Cost calculation failed for {provider}/{model}: {e}")
                comparison[provider] = {
                    "model": model,
                    "error": str(e)
                }
        
        return comparison
    
    def get_cost_optimization_recommendations(
        self,
        current_provider: str,
        current_model: str,
        input_tokens: int,
        output_tokens: int,
        quality_requirements: str = "balanced"
    ) -> List[Dict[str, Any]]:
        """
        Get cost optimization recommendations.
        
        Args:
            current_provider: Current provider being used
            current_model: Current model being used
            input_tokens: Input tokens
            output_tokens: Output tokens
            quality_requirements: Quality requirements (high, balanced, cost_optimized)
            
        Returns:
            List of optimization recommendations
        """
        current_cost = self.estimate_request_cost(current_provider, current_model, input_tokens, output_tokens)
        
        recommendations = []
        
        # Model alternatives within same provider
        provider_models = self.pricing_data.get(current_provider.lower(), {})
        for model_name, pricing in provider_models.items():
            if model_name != current_model:
                alt_cost = self.estimate_request_cost(current_provider, model_name, input_tokens, output_tokens)
                savings = current_cost - alt_cost
                
                if savings > 0.0001:  # Savings > $0.0001
                    recommendations.append({
                        "type": "model_alternative",
                        "provider": current_provider,
                        "recommended_model": model_name,
                        "current_cost": current_cost,
                        "recommended_cost": alt_cost,
                        "savings": savings,
                        "savings_percent": (savings / current_cost) * 100
                    })
        
        # Cross-provider alternatives
        alternative_providers = {
            "openai": ["anthropic", "groq"],
            "anthropic": ["openai", "groq"], 
            "groq": ["openai", "together"],
            "together": ["groq", "cohere"]
        }
        
        for alt_provider in alternative_providers.get(current_provider, []):
            # Use most cost-effective model for comparison
            alt_models = list(self.pricing_data.get(alt_provider, {}).keys())
            if not alt_models:
                continue
                
            # Find cheapest model in alternative provider
            cheapest_model = None
            cheapest_cost = float('inf')
            
            for model in alt_models[:3]:  # Check first 3 models
                try:
                    cost = self.estimate_request_cost(alt_provider, model, input_tokens, output_tokens)
                    if cost < cheapest_cost:
                        cheapest_cost = cost
                        cheapest_model = model
                except:
                    continue
            
            if cheapest_model and cheapest_cost < current_cost:
                savings = current_cost - cheapest_cost
                recommendations.append({
                    "type": "provider_alternative",
                    "current_provider": current_provider,
                    "recommended_provider": alt_provider,
                    "recommended_model": cheapest_model,
                    "current_cost": current_cost,
                    "recommended_cost": cheapest_cost,
                    "savings": savings,
                    "savings_percent": (savings / current_cost) * 100
                })
        
        # Sort by savings (highest first)
        recommendations.sort(key=lambda x: x["savings"], reverse=True)
        
        return recommendations[:5]  # Return top 5 recommendations
    
    def _get_default_rates(self, provider: str) -> Dict[str, float]:
        """Get default pricing rates for unknown models."""
        defaults = {
            "openai": {"input": 1.50, "output": 2.00},
            "anthropic": {"input": 8.00, "output": 24.00},
            "vertex": {"input": 1.00, "output": 1.00},
            "groq": {"input": 0.30, "output": 0.30},
            "together": {"input": 0.50, "output": 0.50},
            "cohere": {"input": 1.00, "output": 2.00}
        }
        return defaults.get(provider.lower(), {"input": 2.00, "output": 4.00})
    
    def get_model_pricing(self, provider: str, model: str) -> Optional[ModelPricing]:
        """Get pricing information for a specific model."""
        return self.pricing_data.get(provider.lower(), {}).get(model)
    
    def list_supported_providers(self) -> List[str]:
        """Get list of all supported providers."""
        return list(self.pricing_data.keys())
    
    def list_provider_models(self, provider: str) -> List[str]:
        """Get list of supported models for a provider."""
        return list(self.pricing_data.get(provider.lower(), {}).keys())
    
    def get_tier_information(self, tier: PricingTier) -> GatewayPricingTier:
        """Get detailed information about a pricing tier."""
        return self.gateway_tiers[tier]
    
    def calculate_monthly_projection(
        self,
        daily_requests: int,
        avg_input_tokens: int,
        avg_output_tokens: int,
        provider_distribution: Dict[str, float],  # Provider -> percentage
        model_preferences: Dict[str, str],
        tier: PricingTier = PricingTier.FREE
    ) -> Dict[str, Any]:
        """
        Calculate monthly cost projection based on usage patterns.
        
        Args:
            daily_requests: Average daily requests
            avg_input_tokens: Average input tokens per request
            avg_output_tokens: Average output tokens per request
            provider_distribution: Distribution of requests across providers
            model_preferences: Preferred model for each provider
            tier: Helicone pricing tier
            
        Returns:
            Monthly cost projection breakdown
        """
        monthly_requests = daily_requests * 30
        
        # Calculate provider costs
        total_provider_cost = 0.0
        provider_breakdown = {}
        
        for provider, percentage in provider_distribution.items():
            provider_requests = int(monthly_requests * percentage)
            provider_tokens_in = provider_requests * avg_input_tokens
            provider_tokens_out = provider_requests * avg_output_tokens
            
            model = model_preferences.get(provider, "default")
            input_cost, output_cost, provider_cost = self.calculate_provider_cost(
                provider, model, provider_tokens_in, provider_tokens_out
            )
            
            total_provider_cost += provider_cost
            provider_breakdown[provider] = {
                "requests": provider_requests,
                "cost": provider_cost,
                "model": model
            }
        
        # Calculate Helicone gateway costs
        monthly_fee, overage_cost = self.calculate_helicone_cost(monthly_requests, tier)
        total_helicone_cost = monthly_fee + overage_cost
        
        total_monthly_cost = total_provider_cost + total_helicone_cost
        
        return {
            "monthly_requests": monthly_requests,
            "provider_costs": {
                "total": total_provider_cost,
                "breakdown": provider_breakdown
            },
            "helicone_costs": {
                "monthly_fee": monthly_fee,
                "overage_cost": overage_cost,
                "total": total_helicone_cost
            },
            "total_monthly_cost": total_monthly_cost,
            "cost_per_request": total_monthly_cost / max(monthly_requests, 1),
            "tier": tier.value,
            "projection_date": datetime.utcnow().isoformat()
        }

# Convenience functions for common calculations
def quick_cost_estimate(provider: str, model: str, tokens: int) -> float:
    """Quick cost estimate for simple use cases."""
    calc = HeliconePricingCalculator()
    return calc.estimate_request_cost(provider, model, tokens, tokens // 2)

def compare_providers(providers: List[str], tokens: int = 1000) -> Dict[str, float]:
    """Quick provider cost comparison."""
    calc = HeliconePricingCalculator()
    costs = {}
    
    for provider in providers:
        models = calc.list_provider_models(provider)
        if models:
            cost = calc.estimate_request_cost(provider, models[0], tokens, tokens // 2)
            costs[provider] = cost
    
    return costs

__all__ = [
    "HeliconePricingCalculator", 
    "ModelPricing",
    "GatewayPricingTier",
    "PricingTier",
    "quick_cost_estimate",
    "compare_providers"
]