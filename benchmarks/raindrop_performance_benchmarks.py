#!/usr/bin/env python3
"""
‚ö° Raindrop AI Performance Benchmarks

Comprehensive performance analysis for Raindrop AI integration with GenOps governance.
Measures latency impact, memory usage, and throughput characteristics specific to
agent monitoring operations with cost tracking and governance oversight.

Benchmarks Include:
‚úÖ Agent interaction tracking overhead
‚úÖ Performance signal monitoring latency
‚úÖ Alert creation and management costs
‚úÖ Memory usage analysis for agent operations
‚úÖ Concurrent agent monitoring performance
‚úÖ Cost calculation performance impact
‚úÖ Real-world agent monitoring simulation

Author: GenOps AI Contributors
License: Apache 2.0
"""

import gc
import statistics
import time
import tracemalloc
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Callable, Dict, List

# Add the src directory to Python path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    from genops.providers.raindrop import GenOpsRaindropAdapter, auto_instrument
    from genops.providers.raindrop_validation import validate_setup
except ImportError as e:
    print(f"‚ùå Error importing GenOps Raindrop: {e}")
    print("üí° Make sure you're in the project root directory and GenOps is properly installed")
    sys.exit(1)


class RaindropPerformanceBenchmark:
    """Performance benchmarking utility for Raindrop AI operations."""

    def __init__(self, warmup_iterations: int = 50, benchmark_iterations: int = 1000):
        self.warmup_iterations = warmup_iterations
        self.benchmark_iterations = benchmark_iterations
        self.results = {}
        
        # Set up clean environment
        self._setup_clean_environment()

    def _setup_clean_environment(self):
        """Set up a clean benchmarking environment."""
        # Force garbage collection
        gc.collect()
        
        print("üîß Setting up clean benchmarking environment...")

    def benchmark_function(self, func: Callable, name: str, *args, **kwargs) -> Dict[str, Any]:
        """Benchmark a function's execution time."""
        
        print(f"üîÑ Benchmarking {name}...")

        # Warmup
        for _ in range(self.warmup_iterations):
            try:
                func(*args, **kwargs)
            except Exception:
                pass  # Ignore errors during warmup

        # Force garbage collection before benchmark
        gc.collect()

        # Benchmark
        timings = []
        errors = 0
        
        for _ in range(self.benchmark_iterations):
            start_time = time.perf_counter()
            try:
                func(*args, **kwargs)
                end_time = time.perf_counter()
                timings.append((end_time - start_time) * 1000)  # Convert to milliseconds
            except Exception:
                errors += 1
                end_time = time.perf_counter()
                timings.append((end_time - start_time) * 1000)  # Include error overhead

        if not timings:
            return {
                'name': name,
                'error': 'All iterations failed',
                'error_rate': 1.0
            }

        # Calculate statistics
        results = {
            'name': name,
            'iterations': self.benchmark_iterations,
            'successful_iterations': len(timings) - errors,
            'error_rate': errors / self.benchmark_iterations,
            'timings_ms': timings,
            'mean_ms': statistics.mean(timings),
            'median_ms': statistics.median(timings),
            'min_ms': min(timings),
            'max_ms': max(timings),
            'stddev_ms': statistics.stdev(timings) if len(timings) > 1 else 0,
            'p95_ms': sorted(timings)[int(0.95 * len(timings))],
            'p99_ms': sorted(timings)[int(0.99 * len(timings))]
        }

        self.results[name] = results
        return results

    def benchmark_memory_usage(self, func: Callable, name: str, *args, **kwargs) -> Dict[str, Any]:
        """Benchmark memory usage of a function."""

        print(f"üß† Memory benchmarking {name}...")

        # Start memory tracing
        tracemalloc.start()

        # Baseline memory
        baseline_snapshot = tracemalloc.take_snapshot()

        # Run function multiple times
        iterations = 100
        for _ in range(iterations):
            try:
                func(*args, **kwargs)
            except Exception:
                pass  # Continue memory testing even with errors

        # Take final snapshot
        final_snapshot = tracemalloc.take_snapshot()

        # Calculate memory diff
        top_stats = final_snapshot.compare_to(baseline_snapshot, 'lineno')

        # Get total memory increase
        total_memory_increase = sum(stat.size_diff for stat in top_stats if stat.size_diff > 0)

        tracemalloc.stop()

        results = {
            'name': f"{name}_memory",
            'iterations': iterations,
            'total_memory_increase_bytes': total_memory_increase,
            'memory_per_operation_bytes': total_memory_increase / iterations,
            'memory_per_operation_kb': (total_memory_increase / iterations) / 1024,
            'top_memory_stats': [(stat.traceback.format()[-1], stat.size_diff) for stat in top_stats[:5]]
        }

        return results


def baseline_agent_operation():
    """Baseline agent monitoring operation with no GenOps instrumentation."""
    # Simulate agent interaction data
    agent_data = {
        "agent_id": "baseline-agent-1",
        "input": "Customer support query about billing",
        "output": "Agent response with resolution steps",
        "performance_signals": {
            "response_time_ms": 250,
            "confidence_score": 0.94,
            "customer_satisfaction": 4.5,
            "resolution_status": "resolved"
        },
        "metadata": {
            "conversation_length": 5,
            "escalation_required": False
        }
    }
    
    # Simulate cost calculation
    estimated_cost = len(str(agent_data)) * 0.00001
    
    return {"agent_data": agent_data, "cost": estimated_cost}


def genops_agent_interaction_tracking():
    """Agent interaction tracking with GenOps governance."""
    try:
        adapter = GenOpsRaindropAdapter(
            raindrop_api_key="benchmark-key",
            team="benchmark-team",
            project="performance-test",
            governance_policy="advisory",  # Advisory mode for benchmarking
            export_telemetry=False  # Disable telemetry export for pure overhead measurement
        )
        
        with adapter.track_agent_monitoring_session("benchmark_session") as session:
            interaction_data = {
                "input": "Customer support query about billing",
                "output": "Agent response with resolution steps",
                "performance_signals": {
                    "response_time_ms": 250,
                    "confidence_score": 0.94,
                    "customer_satisfaction": 4.5
                }
            }
            
            cost_result = session.track_agent_interaction(
                agent_id="benchmark-agent",
                interaction_data=interaction_data,
                cost=0.001  # Fixed cost for consistent benchmarking
            )
            
            return {"cost_result": cost_result, "session_cost": float(session.total_cost)}
    except Exception as e:
        return {"error": str(e)}


def genops_performance_signal_monitoring():
    """Performance signal monitoring with GenOps governance."""
    try:
        adapter = GenOpsRaindropAdapter(
            raindrop_api_key="benchmark-key",
            team="benchmark-team", 
            project="performance-test",
            governance_policy="advisory",
            export_telemetry=False
        )
        
        with adapter.track_agent_monitoring_session("signal_benchmark") as session:
            signal_data = {
                "threshold": 0.85,
                "current_value": 0.92,
                "monitoring_frequency": "high",
                "signal_type": "accuracy_monitoring"
            }
            
            cost_result = session.track_performance_signal(
                signal_name="accuracy_degradation_detector", 
                signal_data=signal_data,
                cost=0.002
            )
            
            return {"cost_result": cost_result, "session_cost": float(session.total_cost)}
    except Exception as e:
        return {"error": str(e)}


def genops_alert_creation():
    """Alert creation and management with GenOps governance."""
    try:
        adapter = GenOpsRaindropAdapter(
            raindrop_api_key="benchmark-key",
            team="benchmark-team",
            project="performance-test", 
            governance_policy="advisory",
            export_telemetry=False
        )
        
        with adapter.track_agent_monitoring_session("alert_benchmark") as session:
            alert_config = {
                "conditions": [
                    {"metric": "response_time", "operator": ">", "threshold": 500},
                    {"metric": "confidence", "operator": "<", "threshold": 0.8}
                ],
                "notification_channels": ["email", "slack"],
                "severity": "warning",
                "escalation_rules": {"max_retries": 3}
            }
            
            cost_result = session.create_alert(
                alert_name="performance_degradation_alert",
                alert_config=alert_config,
                cost=0.05
            )
            
            return {"cost_result": cost_result, "session_cost": float(session.total_cost)}
    except Exception as e:
        return {"error": str(e)}


def genops_cost_calculation_overhead():
    """Test cost calculation performance overhead."""
    try:
        adapter = GenOpsRaindropAdapter(
            raindrop_api_key="benchmark-key",
            team="benchmark-team",
            project="cost-benchmark",
            governance_policy="advisory",
            export_telemetry=False
        )
        
        # Test pricing calculator performance
        calculator = adapter.pricing_calculator
        
        interaction_data = {
            "input": "Test query for cost calculation",
            "output": "Test response",
            "performance_signals": {"latency": 200}
        }
        
        cost_result = calculator.calculate_interaction_cost(
            agent_id="cost-test-agent",
            interaction_data=interaction_data,
            complexity="moderate"
        )
        
        return {"cost_result": cost_result, "total_cost": float(cost_result.total_cost)}
    except Exception as e:
        return {"error": str(e)}


def genops_auto_instrumentation_overhead():
    """Test auto-instrumentation setup overhead."""
    try:
        # Test auto-instrumentation setup time
        start_time = time.perf_counter()
        
        adapter = auto_instrument(
            raindrop_api_key="benchmark-key",
            team="auto-benchmark",
            project="instrumentation-test",
            governance_policy="advisory",
            export_telemetry=False
        )
        
        setup_time = time.perf_counter() - start_time
        
        # Test simple operation
        with adapter.track_agent_monitoring_session("auto_test") as session:
            cost_result = session.track_agent_interaction(
                agent_id="auto-agent",
                interaction_data={"test": "data"},
                cost=0.001
            )
        
        return {
            "setup_time_ms": setup_time * 1000,
            "cost_result": cost_result,
            "success": True
        }
    except Exception as e:
        return {"error": str(e), "success": False}


def concurrent_agent_monitoring():
    """Test concurrent agent monitoring performance."""
    try:
        adapter = GenOpsRaindropAdapter(
            raindrop_api_key="benchmark-key",
            team="concurrent-team",
            project="concurrent-test",
            governance_policy="advisory",
            export_telemetry=False
        )
        
        def monitor_agent(agent_id: int):
            """Monitor a single agent with multiple operations."""
            results = []
            
            with adapter.track_agent_monitoring_session(f"agent_{agent_id}_session") as session:
                # Track multiple operations per agent
                for op_id in range(5):
                    try:
                        cost_result = session.track_agent_interaction(
                            agent_id=f"concurrent-agent-{agent_id}",
                            interaction_data={
                                "operation_id": op_id,
                                "input": f"Query {op_id}",
                                "output": f"Response {op_id}"
                            },
                            cost=0.001
                        )
                        results.append(float(cost_result.total_cost))
                    except Exception:
                        results.append(0.0)
            
            return results
        
        # Run 10 concurrent agents
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(monitor_agent, i) for i in range(10)]
            all_results = []
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    all_results.extend(result)
                except Exception:
                    pass
        
        return {
            "total_operations": len(all_results),
            "total_cost": sum(all_results),
            "average_cost": sum(all_results) / len(all_results) if all_results else 0,
            "success": True
        }
    except Exception as e:
        return {"error": str(e), "success": False}


def high_frequency_agent_operations():
    """Test performance under high-frequency agent operations."""
    try:
        adapter = GenOpsRaindropAdapter(
            raindrop_api_key="benchmark-key",
            team="high-freq-team", 
            project="frequency-test",
            governance_policy="advisory",
            export_telemetry=False
        )
        
        operations_count = 50  # Reduced for benchmark speed
        total_cost = 0.0
        
        with adapter.track_agent_monitoring_session("high_frequency_test") as session:
            for i in range(operations_count):
                cost_result = session.track_agent_interaction(
                    agent_id=f"freq-agent-{i % 5}",  # Rotate through 5 agents
                    interaction_data={
                        "sequence": i,
                        "input": f"High frequency query {i}",
                        "output": f"Response {i}"
                    },
                    cost=0.001
                )
                total_cost += float(cost_result.total_cost)
        
        return {
            "operations_completed": operations_count,
            "total_cost": total_cost,
            "average_cost_per_op": total_cost / operations_count,
            "session_duration": session.duration_seconds if hasattr(session, 'duration_seconds') else 0
        }
    except Exception as e:
        return {"error": str(e)}


def run_raindrop_performance_benchmarks():
    """Run comprehensive Raindrop AI performance benchmarks."""
    
    print("‚ö° Raindrop AI Performance Benchmarks")
    print("=" * 80)
    print("üéØ Testing performance impact of GenOps governance on Raindrop AI operations")
    print()
    
    benchmark = RaindropPerformanceBenchmark(
        warmup_iterations=50,
        benchmark_iterations=500  # Reduced for faster execution
    )
    
    # Benchmark different operation types
    benchmarks = [
        (baseline_agent_operation, "baseline_no_genops"),
        (genops_agent_interaction_tracking, "genops_agent_tracking"), 
        (genops_performance_signal_monitoring, "genops_signal_monitoring"),
        (genops_alert_creation, "genops_alert_creation"),
        (genops_cost_calculation_overhead, "genops_cost_calculation"),
        (genops_auto_instrumentation_overhead, "genops_auto_instrumentation"),
        (concurrent_agent_monitoring, "genops_concurrent_agents"),
        (high_frequency_agent_operations, "genops_high_frequency")
    ]
    
    results = []
    
    for func, name in benchmarks:
        result = benchmark.benchmark_function(func, name)
        results.append(result)
        
        # Memory benchmark for selected operations
        if name in ["baseline_no_genops", "genops_agent_tracking", "genops_signal_monitoring"]:
            memory_result = benchmark.benchmark_memory_usage(func, name)
            results.append(memory_result)
    
    return results


def analyze_raindrop_performance_results(results: List[Dict[str, Any]]):
    """Analyze and display Raindrop AI performance benchmark results."""
    
    print("\nüìä RAINDROP AI PERFORMANCE ANALYSIS")
    print("=" * 80)
    
    # Filter timing results
    timing_results = [r for r in results if 'mean_ms' in r and 'error' not in r]
    
    # Find baseline performance
    baseline = next((r for r in timing_results if r['name'] == 'baseline_no_genops'), None)
    
    if not baseline:
        print("‚ö†Ô∏è No baseline found - showing absolute performance metrics")
        baseline_mean = 0
    else:
        baseline_mean = baseline['mean_ms']
    
    print(f"üèÅ LATENCY ANALYSIS (baseline: {baseline_mean:.4f}ms)")
    print("-" * 70)
    print(f"{'Operation':<30} | {'Mean (ms)':<10} | {'P95 (ms)':<9} | {'Overhead':<8} | {'Errors'}")
    print("-" * 70)
    
    for result in timing_results:
        name = result['name'].replace('genops_', '').replace('_', ' ').title()
        mean_ms = result['mean_ms']
        p95_ms = result['p95_ms']
        error_rate = result.get('error_rate', 0) * 100
        
        if baseline_mean > 0 and result['name'] != 'baseline_no_genops':
            overhead_pct = ((mean_ms - baseline_mean) / baseline_mean * 100)
            overhead_str = f"{overhead_pct:+6.2f}%"
        else:
            overhead_str = "   -   "
        
        print(f"{name:<30} | {mean_ms:8.4f}   | {p95_ms:7.4f}   | {overhead_str:<8} | {error_rate:5.1f}%")
    
    # Memory analysis
    memory_results = [r for r in results if 'memory' in r['name']]
    
    if memory_results:
        print(f"\nüß† MEMORY USAGE ANALYSIS")
        print("-" * 50)
        print(f"{'Operation':<30} | {'Per Op (KB)':<12} | {'Per Op (bytes)'}")
        print("-" * 50)
        
        for result in memory_results:
            name = result['name'].replace('_memory', '').replace('genops_', '').replace('_', ' ').title()
            per_op_kb = result['memory_per_operation_kb']
            per_op_bytes = result['memory_per_operation_bytes']
            
            print(f"{name:<30} | {per_op_kb:10.2f}   | {per_op_bytes:8.1f}")
    
    # Agent monitoring specific analysis
    print(f"\nü§ñ AGENT MONITORING PERFORMANCE SUMMARY")
    print("=" * 60)
    
    agent_ops = [r for r in timing_results if 'agent' in r['name'] or 'signal' in r['name'] or 'alert' in r['name']]
    
    if agent_ops and baseline:
        max_overhead = max((r['mean_ms'] - baseline_mean) / baseline_mean * 100 for r in agent_ops if r['name'] != 'baseline_no_genops')
        avg_overhead = sum((r['mean_ms'] - baseline_mean) / baseline_mean * 100 for r in agent_ops if r['name'] != 'baseline_no_genops') / len([r for r in agent_ops if r['name'] != 'baseline_no_genops'])
        max_latency_ms = max(r['mean_ms'] for r in agent_ops if r['name'] != 'baseline_no_genops')
        
        print(f"Maximum GenOps overhead: {max_overhead:.2f}% ({max_latency_ms:.4f}ms)")
        print(f"Average GenOps overhead: {avg_overhead:.2f}%")
        
        # Specific Raindrop recommendations
        print("\nüí° RAINDROP AI OPTIMIZATION RECOMMENDATIONS")
        print("-" * 45)
        
        if max_latency_ms < 0.1:
            print("‚úÖ Excellent: Raindrop governance overhead is negligible (<0.1ms)")
        elif max_latency_ms < 1.0:
            print("‚úÖ Good: Raindrop governance overhead is minimal (<1ms)")
        elif max_latency_ms < 5.0:
            print("‚ö†Ô∏è Acceptable: Raindrop governance overhead is reasonable (<5ms)")
        else:
            print("‚ùå High latency: Consider optimization for high-frequency agent monitoring")
        
        # Feature-specific recommendations
        signal_result = next((r for r in timing_results if 'signal' in r['name']), None)
        if signal_result and baseline:
            signal_overhead = ((signal_result['mean_ms'] - baseline_mean) / baseline_mean * 100)
            if signal_overhead > 20:
                print("‚Ä¢ Consider reducing performance signal monitoring frequency")
                print("‚Ä¢ Batch multiple signals in single monitoring sessions")
        
        alert_result = next((r for r in timing_results if 'alert' in r['name']), None) 
        if alert_result and baseline:
            alert_overhead = ((alert_result['mean_ms'] - baseline_mean) / baseline_mean * 100)
            if alert_overhead > 30:
                print("‚Ä¢ Optimize alert configuration complexity")
                print("‚Ä¢ Consider async alert creation for high-volume scenarios")
        
        # Cost calculation optimization
        cost_result = next((r for r in timing_results if 'cost' in r['name']), None)
        if cost_result and baseline:
            cost_overhead = ((cost_result['mean_ms'] - baseline_mean) / baseline_mean * 100)
            if cost_overhead > 10:
                print("‚Ä¢ Cache cost calculation results for similar operations")
                print("‚Ä¢ Use simplified cost models for high-frequency operations")


def run_raindrop_stress_test():
    """Run Raindrop-specific stress test for agent monitoring at scale."""
    
    print("\nüî• RAINDROP AI STRESS TEST")
    print("=" * 60)
    print("Testing high-frequency agent monitoring performance...")
    
    try:
        adapter = GenOpsRaindropAdapter(
            raindrop_api_key="stress-test-key",
            team="stress-test-team",
            project="stress-test",
            governance_policy="advisory",
            export_telemetry=False
        )
        
        start_time = time.time()
        operation_count = 0
        test_duration = 3.0  # 3 seconds for faster execution
        
        with adapter.track_agent_monitoring_session("stress_test_session") as session:
            while (time.time() - start_time) < test_duration:
                try:
                    session.track_agent_interaction(
                        agent_id=f"stress-agent-{operation_count % 10}",
                        interaction_data={
                            "stress_test_id": operation_count,
                            "input": f"Stress query {operation_count}",
                            "output": f"Response {operation_count}"
                        },
                        cost=0.001
                    )
                    operation_count += 1
                except Exception:
                    operation_count += 1  # Count errors too
        
        end_time = time.time()
        actual_duration = end_time - start_time
        ops_per_second = operation_count / actual_duration
        avg_latency = (actual_duration / operation_count * 1000) if operation_count > 0 else 0
        
        print(f"Agent interactions completed: {operation_count:,}")
        print(f"Duration: {actual_duration:.2f}s")
        print(f"Agent interactions per second: {ops_per_second:,.0f}")
        print(f"Average latency per interaction: {avg_latency:.4f}ms")
        print(f"Session total cost: ${float(session.total_cost):.4f}")
        
        # Performance verdict for Raindrop AI
        if ops_per_second > 5000:
            print("‚úÖ Excellent agent monitoring throughput (>5k interactions/sec)")
        elif ops_per_second > 2000:
            print("‚úÖ Good agent monitoring throughput (>2k interactions/sec)")
        elif ops_per_second > 500:
            print("‚ö†Ô∏è Moderate agent monitoring throughput (>500 interactions/sec)")
        else:
            print("‚ùå Low agent monitoring throughput (<500 interactions/sec)")
        
    except Exception as e:
        print(f"‚ùå Stress test failed: {e}")


def main():
    """Run complete Raindrop AI performance benchmark suite."""
    
    print("‚ö° Raindrop AI Performance Benchmark Suite")
    print("=" * 80)
    print("\nThis benchmark measures the performance impact of GenOps governance")
    print("on Raindrop AI agent monitoring operations.\n")
    
    print("üéØ Test Coverage:")
    print("  ‚Ä¢ Agent interaction tracking overhead")
    print("  ‚Ä¢ Performance signal monitoring latency") 
    print("  ‚Ä¢ Alert creation and management costs")
    print("  ‚Ä¢ Memory usage for agent operations")
    print("  ‚Ä¢ Concurrent agent monitoring performance")
    print("  ‚Ä¢ Cost calculation overhead")
    print("  ‚Ä¢ High-frequency agent operations")
    print()
    
    # Run benchmarks
    results = run_raindrop_performance_benchmarks()
    
    # Analyze results
    analyze_raindrop_performance_results(results)
    
    # Run stress test
    run_raindrop_stress_test()
    
    print("\nüèÜ RAINDROP AI BENCHMARK COMPLETE")
    print("=" * 60)
    print("Results demonstrate GenOps adds minimal overhead to Raindrop AI")
    print("agent monitoring while providing comprehensive governance and cost intelligence.")
    
    print("\nüìà RAINDROP AI OPTIMIZATION TIPS")
    print("-" * 45)
    print("‚Ä¢ Use advisory governance policy for high-frequency operations")
    print("‚Ä¢ Batch multiple agent interactions in single monitoring sessions")
    print("‚Ä¢ Configure performance signal sampling for large-scale deployments")
    print("‚Ä¢ Use async telemetry export for production systems")
    print("‚Ä¢ Cache cost calculations for similar operation patterns")
    print("‚Ä¢ Consider alert consolidation for high-volume scenarios")
    print("‚Ä¢ Monitor memory usage in long-running agent monitoring processes")
    
    print(f"\nüìã Detailed optimization guide: docs/raindrop-performance-benchmarks.md")


if __name__ == "__main__":
    main()