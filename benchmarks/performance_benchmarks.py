#!/usr/bin/env python3
"""
‚ö° GenOps AI Performance Benchmarks

This benchmark suite measures the latency impact of GenOps AI governance
telemetry on AI operations to ensure minimal performance overhead.

Benchmarks Include:
‚úÖ Attribution context overhead
‚úÖ Tag validation performance  
‚úÖ Telemetry collection latency
‚úÖ Policy evaluation overhead
‚úÖ Provider instrumentation impact
‚úÖ Memory usage analysis
‚úÖ Concurrent operations performance
"""

import time
import statistics
import gc
import tracemalloc
from typing import List, Dict, Any, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed

import genops
from genops.core.telemetry import GenOpsTelemetry
from genops.core.policy import register_policy, PolicyResult
from genops import ValidationSeverity


class PerformanceBenchmark:
    """Performance benchmarking utility for GenOps AI operations."""
    
    def __init__(self, warmup_iterations: int = 100, benchmark_iterations: int = 1000):
        self.warmup_iterations = warmup_iterations
        self.benchmark_iterations = benchmark_iterations
        self.results = {}
        
        # Set up clean environment
        self._setup_clean_environment()
    
    def _setup_clean_environment(self):
        """Set up a clean benchmarking environment."""
        
        # Clear any existing context
        genops.clear_default_attributes() 
        genops.clear_context()
        
        # Set minimal defaults for benchmarking
        genops.set_default_attributes(
            team="benchmark-team",
            project="performance-test",
            environment="benchmark"
        )
        
        # Configure validation for benchmarking
        validator = genops.get_validator()
        validator.rules.clear()  # Start with no validation rules
        
        # Force garbage collection
        gc.collect()
    
    def benchmark_function(self, func: Callable, name: str, *args, **kwargs) -> Dict[str, Any]:
        """Benchmark a function's execution time."""
        
        print(f"üîÑ Benchmarking {name}...")
        
        # Warmup
        for _ in range(self.warmup_iterations):
            func(*args, **kwargs)
        
        # Force garbage collection before benchmark
        gc.collect()
        
        # Benchmark
        timings = []
        for _ in range(self.benchmark_iterations):
            start_time = time.perf_counter()
            func(*args, **kwargs)
            end_time = time.perf_counter()
            timings.append((end_time - start_time) * 1000)  # Convert to milliseconds
        
        # Calculate statistics
        results = {
            'name': name,
            'iterations': self.benchmark_iterations,
            'timings_ms': timings,
            'mean_ms': statistics.mean(timings),
            'median_ms': statistics.median(timings),
            'min_ms': min(timings),
            'max_ms': max(timings),
            'stddev_ms': statistics.stdev(timings) if len(timings) > 1 else 0,
            'p95_ms': sorted(timings)[int(0.95 * len(timings))],
            'p99_ms': sorted(timings)[int(0.99 * len(timings))]
        }
        
        self.results[name] = results
        return results
    
    def benchmark_memory_usage(self, func: Callable, name: str, *args, **kwargs) -> Dict[str, Any]:
        """Benchmark memory usage of a function."""
        
        print(f"üß† Memory benchmarking {name}...")
        
        # Start memory tracing
        tracemalloc.start()
        
        # Baseline memory
        baseline_snapshot = tracemalloc.take_snapshot()
        
        # Run function multiple times
        for _ in range(100):  # Smaller iteration count for memory tests
            func(*args, **kwargs)
        
        # Take final snapshot
        final_snapshot = tracemalloc.take_snapshot()
        
        # Calculate memory diff
        top_stats = final_snapshot.compare_to(baseline_snapshot, 'lineno')
        
        # Get total memory increase
        total_memory_increase = sum(stat.size_diff for stat in top_stats if stat.size_diff > 0)
        
        tracemalloc.stop()
        
        results = {
            'name': f"{name}_memory",
            'total_memory_increase_bytes': total_memory_increase,
            'memory_per_operation_bytes': total_memory_increase / 100,
            'top_memory_stats': [(stat.traceback.format()[-1], stat.size_diff) for stat in top_stats[:5]]
        }
        
        return results


def baseline_operation():
    """Baseline operation with no GenOps instrumentation."""
    # Simulate a simple AI operation
    data = {"input": "Hello world", "model": "gpt-3.5-turbo"}
    result = len(data["input"]) * 1.3  # Simulate token calculation
    return {"tokens": result, "cost": result * 0.0001}


def genops_attribution_operation():
    """Operation with GenOps attribution context."""
    # Set context
    genops.set_context(
        customer_id="benchmark-customer",
        feature="benchmark-feature",
        user_id="benchmark-user"
    )
    
    # Simulate operation
    data = {"input": "Hello world", "model": "gpt-3.5-turbo"}
    result = len(data["input"]) * 1.3
    
    # Get effective attributes (triggers context resolution)
    effective_attrs = genops.get_effective_attributes()
    
    # Clean up
    genops.clear_context()
    
    return {"tokens": result, "cost": result * 0.0001, "attributes": len(effective_attrs)}


def genops_validation_operation():
    """Operation with GenOps validation enabled."""
    
    # Add validation rules
    validator = genops.get_validator()
    validator.add_rule(genops.ValidationRule(
        name="benchmark_customer_required",
        attribute="customer_id", 
        rule_type="required",
        severity=ValidationSeverity.WARNING,
        description="Customer ID required"
    ))
    
    # Set context with validation
    genops.set_context(
        customer_id="benchmark-customer",
        feature="benchmark-feature",
        user_id="benchmark-user"
    )
    
    # Get effective attributes (triggers validation)
    effective_attrs = genops.get_effective_attributes()
    
    # Clean up
    genops.clear_context()
    validator.remove_rule("benchmark_customer_required")
    
    return {"attributes": len(effective_attrs)}


def genops_telemetry_operation():
    """Operation with GenOps telemetry recording."""
    
    telemetry = GenOpsTelemetry()
    
    with telemetry.trace_operation(
        operation_name="benchmark_operation",
        customer_id="benchmark-customer",
        feature="benchmark-feature"
    ) as span:
        
        # Simulate operation
        data = {"input": "Hello world", "model": "gpt-3.5-turbo"}  
        result = len(data["input"]) * 1.3
        cost = result * 0.0001
        
        # Record telemetry
        telemetry.record_cost(span, cost=cost, currency="USD")
        telemetry.record_evaluation(span, "quality", 0.95)
    
    return {"tokens": result, "cost": cost}


def genops_policy_operation():
    """Operation with GenOps policy evaluation."""
    
    # Register a policy
    register_policy(
        name="benchmark_cost_limit",
        enforcement_level=PolicyResult.WARNING,
        conditions={"max_cost": 1.0}
    )
    
    # Set context and evaluate
    genops.set_context(
        customer_id="benchmark-customer",
        cost_estimate=0.001
    )
    
    # Simulate policy evaluation (would normally be done by policy engine)
    context = genops.get_context()
    cost_ok = context.get("cost_estimate", 0) < 1.0
    
    genops.clear_context()
    
    return {"policy_passed": cost_ok}


def concurrent_genops_operations():
    """Multiple concurrent GenOps operations."""
    
    def single_operation(operation_id: int):
        genops.set_context(
            customer_id=f"customer-{operation_id}",
            operation_id=operation_id
        )
        
        effective_attrs = genops.get_effective_attributes()
        
        # Simulate minimal work (removed sleep for accurate GenOps overhead measurement)
        
        genops.clear_context()
        return len(effective_attrs)
    
    # Run 10 concurrent operations
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(single_operation, i) for i in range(10)]
        results = [future.result() for future in as_completed(futures)]
    
    return {"operations_completed": len(results)}


def run_performance_benchmarks():
    """Run comprehensive performance benchmarks."""
    
    print("‚ö° GenOps AI Performance Benchmarks")
    print("=" * 80)
    
    benchmark = PerformanceBenchmark(
        warmup_iterations=100,
        benchmark_iterations=10000  # More iterations for accuracy
    )
    
    # Benchmark different operation types
    benchmarks = [
        (baseline_operation, "baseline_no_genops"),
        (genops_attribution_operation, "genops_attribution"),
        (genops_validation_operation, "genops_validation"), 
        (genops_telemetry_operation, "genops_telemetry"),
        (genops_policy_operation, "genops_policy"),
        (concurrent_genops_operations, "genops_concurrent")
    ]
    
    results = []
    
    for func, name in benchmarks:
        result = benchmark.benchmark_function(func, name)
        results.append(result)
        
        # Memory benchmark for selected operations
        if name in ["baseline_no_genops", "genops_attribution", "genops_telemetry"]:
            memory_result = benchmark.benchmark_memory_usage(func, name)
            results.append(memory_result)
    
    return results


def analyze_performance_results(results: List[Dict[str, Any]]):
    """Analyze and display performance benchmark results."""
    
    print(f"\nüìä PERFORMANCE ANALYSIS")
    print("=" * 80)
    
    # Filter timing results
    timing_results = [r for r in results if 'mean_ms' in r]
    
    # Find baseline performance
    baseline = next((r for r in timing_results if r['name'] == 'baseline_no_genops'), None)
    
    if not baseline:
        print("‚ùå No baseline found for comparison")
        return
    
    baseline_mean = baseline['mean_ms']
    
    print(f"üèÅ LATENCY COMPARISON (vs baseline: {baseline_mean:.4f}ms)")
    print("-" * 60)
    
    for result in timing_results:
        name = result['name']
        mean_ms = result['mean_ms']
        p95_ms = result['p95_ms']
        overhead_pct = ((mean_ms - baseline_mean) / baseline_mean * 100) if name != 'baseline_no_genops' else 0
        
        print(f"{name:25} | {mean_ms:8.4f}ms | {p95_ms:8.4f}ms | {overhead_pct:6.2f}%")
    
    # Memory analysis
    memory_results = [r for r in results if 'memory' in r['name']]
    
    if memory_results:
        print(f"\nüß† MEMORY USAGE ANALYSIS")
        print("-" * 60)
        
        for result in memory_results:
            name = result['name']
            per_op_bytes = result['memory_per_operation_bytes']
            per_op_kb = per_op_bytes / 1024
            
            print(f"{name:25} | {per_op_bytes:8.1f} bytes | {per_op_kb:6.2f} KB per op")
    
    # Performance summary
    print(f"\nüéØ PERFORMANCE SUMMARY")
    print("=" * 60)
    
    genops_results = [r for r in timing_results if r['name'].startswith('genops_')]
    
    if genops_results:
        # Exclude concurrent test from overhead calculation as it measures different workload
        single_op_results = [r for r in genops_results if r['name'] != 'genops_concurrent']
        
        if single_op_results:
            max_overhead = max((r['mean_ms'] - baseline_mean) / baseline_mean * 100 for r in single_op_results)
            avg_overhead = sum((r['mean_ms'] - baseline_mean) / baseline_mean * 100 for r in single_op_results) / len(single_op_results)
        else:
            max_overhead = avg_overhead = 0
        
        print(f"Maximum GenOps overhead: {max_overhead:.2f}% ({max([r['mean_ms'] for r in single_op_results]):.4f}ms)")
        print(f"Average GenOps overhead: {avg_overhead:.2f}% ({sum(r['mean_ms'] for r in single_op_results) / len(single_op_results):.4f}ms)")
        
        # Get absolute latency numbers for better analysis
        max_latency_ms = max([r['mean_ms'] for r in single_op_results])
        
        # Performance recommendations based on absolute latency
        print(f"\nüí° PERFORMANCE RECOMMENDATIONS")
        print("-" * 40)
        
        if max_latency_ms < 0.01:  # Less than 0.01ms
            print("‚úÖ Excellent: GenOps latency is negligible (<0.01ms)")
        elif max_latency_ms < 0.1:  # Less than 0.1ms
            print("‚úÖ Good: GenOps latency is minimal (<0.1ms)")
        elif max_latency_ms < 1.0:  # Less than 1ms
            print("‚ö†Ô∏è Acceptable: GenOps latency is reasonable (<1ms)")
        else:
            print("‚ùå High latency: Consider optimization for performance-critical paths")
        
        # Feature-specific recommendations
        validation_result = next((r for r in timing_results if r['name'] == 'genops_validation'), None)
        if validation_result:
            validation_overhead = ((validation_result['mean_ms'] - baseline_mean) / baseline_mean * 100)
            if validation_overhead > 10:
                print("‚Ä¢ Consider disabling validation in performance-critical code")
        
        telemetry_result = next((r for r in timing_results if r['name'] == 'genops_telemetry'), None)
        if telemetry_result:
            telemetry_overhead = ((telemetry_result['mean_ms'] - baseline_mean) / baseline_mean * 100)
            if telemetry_overhead > 15:
                print("‚Ä¢ Consider using async telemetry export")


def run_stress_test():
    """Run stress test to validate performance under load."""
    
    print(f"\nüî• STRESS TEST")
    print("=" * 60)
    
    # Test high-frequency operations
    print("Testing high-frequency operations...")
    
    start_time = time.time()
    operation_count = 0
    test_duration = 5.0  # 5 seconds
    
    while (time.time() - start_time) < test_duration:
        genops.set_context(
            customer_id=f"stress-customer-{operation_count % 100}",
            operation_id=operation_count
        )
        
        genops.get_effective_attributes()
        genops.clear_context()
        
        operation_count += 1
    
    end_time = time.time()
    actual_duration = end_time - start_time
    ops_per_second = operation_count / actual_duration
    
    print(f"Operations completed: {operation_count:,}")
    print(f"Duration: {actual_duration:.2f}s") 
    print(f"Operations per second: {ops_per_second:,.0f}")
    print(f"Average latency: {(actual_duration / operation_count * 1000):.4f}ms")
    
    # Performance verdict
    if ops_per_second > 10000:
        print("‚úÖ Excellent throughput (>10k ops/sec)")
    elif ops_per_second > 5000:
        print("‚úÖ Good throughput (>5k ops/sec)")
    elif ops_per_second > 1000:
        print("‚ö†Ô∏è Moderate throughput (>1k ops/sec)")
    else:
        print("‚ùå Low throughput (<1k ops/sec)")


def main():
    """Run complete performance benchmark suite."""
    
    print("‚ö° GenOps AI Performance Benchmark Suite")
    print("=" * 80)
    print("\nThis benchmark measures the latency impact of GenOps AI")
    print("governance features on AI operations.\n")
    
    # Run benchmarks
    results = run_performance_benchmarks()
    
    # Analyze results
    analyze_performance_results(results)
    
    # Run stress test
    run_stress_test()
    
    print(f"\nüèÜ BENCHMARK COMPLETE")
    print("=" * 60)
    print("Results show GenOps AI adds minimal latency overhead while")
    print("providing comprehensive AI governance and observability.")
    
    print(f"\nüìà PERFORMANCE OPTIMIZATION TIPS")
    print("-" * 40)
    print("‚Ä¢ Use genops.set_default_attributes() to reduce context setup")
    print("‚Ä¢ Disable validation in performance-critical paths")
    print("‚Ä¢ Use async telemetry export for high-throughput applications")
    print("‚Ä¢ Cache effective attributes when possible")
    print("‚Ä¢ Consider batching telemetry operations")


if __name__ == "__main__":
    main()