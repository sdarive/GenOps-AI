#!/usr/bin/env python3
"""
üéØ GenOps + Ollama: 30-Second Confidence Builder

GOAL: Prove GenOps tracks your local Ollama models with zero code changes
TIME: 30 seconds
WHAT YOU'LL LEARN: GenOps automatically tracks local model costs and performance

This is your "hello world" for GenOps + Ollama integration.
Just run it and see GenOps tracking in action!

Prerequisites:
- Ollama installed and running: `ollama serve`
- At least one model: `ollama pull llama3.2:1b`
"""

import os
import sys
import time

def main():
    print("üöÄ GenOps + Ollama: 30-Second Confidence Builder")
    print("="*55)
    
    # Step 1: Validate setup
    print("\nüìã Step 1: Validating Ollama setup...")
    
    try:
        from genops.providers.ollama.validation import quick_validate
        
        if quick_validate():
            print("‚úÖ Ollama server is running and accessible")
        else:
            print("‚ùå Ollama validation failed")
            print("\nüîß Quick fixes:")
            print("   1. Start Ollama: ollama serve")  
            print("   2. Pull a model: ollama pull llama3.2:1b")
            print("   3. Check connection: curl http://localhost:11434/api/version")
            return False
            
    except Exception as e:
        print(f"‚ùå Setup validation error: {e}")
        print("\nüí° Install GenOps: pip install genops-ai[ollama]")
        return False
    
    # Step 2: Enable GenOps tracking
    print("\n‚ö° Step 2: Enabling GenOps tracking...")
    
    try:
        from genops.providers.ollama import auto_instrument
        
        # Enable automatic tracking with team attribution
        auto_instrument(
            team="quickstart-demo",
            project="30-second-test",
            environment="development"
        )
        print("‚úÖ GenOps auto-instrumentation enabled")
        
    except Exception as e:
        print(f"‚ùå Auto-instrumentation error: {e}")
        return False
    
    # Step 3: Test with existing Ollama code
    print("\nü§ñ Step 3: Testing with your existing Ollama code...")
    
    try:
        import ollama
        
        # Your existing Ollama code - NO CHANGES NEEDED!
        # GenOps will automatically track this
        print("   Generating text with local model...")
        
        start_time = time.time()
        response = ollama.generate(
            model="llama3.2:1b",  # Change to your available model
            prompt="What is GenOps in one sentence?"
        )
        duration = time.time() - start_time
        
        print(f"‚úÖ Generation successful!")
        print(f"   üìù Response: {response['response'][:100]}...")
        print(f"   ‚è±Ô∏è  Duration: {duration:.1f}s")
        
    except Exception as e:
        error_str = str(e).lower()
        if "not found" in error_str or "model" in error_str:
            print("‚ùå Model not found")
            print("\nüîß Available models:")
            try:
                models = ollama.list()
                if models.get('models'):
                    for model in models['models'][:3]:
                        print(f"   - {model['name']}")
                    print("\nüí° Update the model name in line 67 to one of the above")
                else:
                    print("   No models found. Pull one: ollama pull llama3.2:1b")
            except:
                print("   Cannot list models. Check Ollama connection.")
            return False
        else:
            print(f"‚ùå Generation error: {e}")
            return False
    
    # Step 4: Show GenOps tracking results
    print("\nüìä Step 4: GenOps tracking results...")
    
    try:
        from genops.providers.ollama import get_resource_monitor, get_model_manager
        
        # Get resource monitoring data
        monitor = get_resource_monitor()
        current_metrics = monitor.get_current_metrics()
        
        if current_metrics:
            print("   üñ•Ô∏è System Resources:")
            print(f"      CPU Usage: {current_metrics.cpu_usage_percent:.1f}%")
            print(f"      Memory: {current_metrics.memory_usage_mb:.0f}MB")
            if current_metrics.gpu_usage_percent > 0:
                print(f"      GPU Usage: {current_metrics.gpu_usage_percent:.1f}%")
        
        # Get model performance data
        manager = get_model_manager()
        performance = manager.get_model_performance_summary()
        
        if performance:
            for model, stats in performance.items():
                if stats.get('total_inferences', 0) > 0:
                    print(f"   ü§ñ Model Performance ({model}):")
                    print(f"      Inferences: {stats.get('total_inferences', 0)}")
                    print(f"      Avg Latency: {stats.get('avg_inference_latency_ms', 0):.0f}ms")
                    if stats.get('cost_per_inference', 0) > 0:
                        print(f"      Infrastructure Cost: ${stats.get('cost_per_inference', 0):.6f}/inference")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Cannot display metrics: {e}")
    
    # Success!
    print("\n" + "="*55)
    print("üéâ SUCCESS! GenOps is now tracking your Ollama usage")
    print("="*55)
    
    print("\n‚úÖ What you just accomplished:")
    print("   ‚Ä¢ GenOps automatically tracked your local model usage")
    print("   ‚Ä¢ Infrastructure costs calculated (GPU/CPU time, electricity)")
    print("   ‚Ä¢ Performance metrics captured (latency, throughput)")
    print("   ‚Ä¢ Team attribution applied (quickstart-demo team)")
    print("   ‚Ä¢ Zero changes to your existing Ollama code!")
    
    print("\nüöÄ Next steps (choose your path):")
    print("   ‚Ä¢ 15 min: Run local_model_optimization.py for cost optimization")
    print("   ‚Ä¢ 30 min: Try ollama_production_deployment.py for enterprise patterns")
    print("   ‚Ä¢ 5 min: Check out the Ollama integration guide")
    
    return True


if __name__ == "__main__":
    try:
        success = main()
        if success:
            sys.exit(0)
        else:
            sys.exit(1)
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Interrupted by user")
        sys.exit(0)
    except Exception as e:
        print(f"\nüí• Unexpected error: {e}")
        print("\nüÜò If this persists:")
        print("   1. Check Ollama is running: ollama serve")
        print("   2. Reinstall GenOps: pip install --upgrade genops-ai[ollama]")
        print("   3. Report issue: https://github.com/KoshiHQ/GenOps-AI/issues")
        sys.exit(1)