#!/usr/bin/env python3
"""
Auto-Instrumentation OpenLLMetry + GenOps Example

This example demonstrates zero-code governance enhancement for existing OpenLLMetry applications.
GenOps automatically adds cost attribution, team tracking, and policy enforcement without
requiring changes to your existing code.

Perfect for teams already using OpenLLMetry who want to add governance intelligence.

Usage:
    python auto_instrumentation.py

Prerequisites:
    pip install genops[traceloop]  # Includes OpenLLMetry
    export OPENAI_API_KEY="your-openai-api-key"
    
    # Optional: For Traceloop commercial platform
    export TRACELOOP_API_KEY="your-traceloop-api-key"
"""

import os
import asyncio
from datetime import datetime


def setup_auto_instrumentation():
    """
    Set up automatic instrumentation that enhances existing OpenLLMetry code
    with GenOps governance without requiring code changes.
    """
    print("‚ö° Auto-Instrumentation Setup")
    print("=" * 30)
    
    try:
        # Import and initialize GenOps auto-instrumentation
        from genops.providers.traceloop import auto_instrument
        
        print("‚úÖ GenOps auto-instrumentation loaded")
        
        # Configure governance context for all operations
        governance_config = {
            "team": "platform-engineering",
            "project": "auto-instrumentation-demo", 
            "environment": "development",
            "cost_center": "engineering-ops",
            "enable_cost_alerts": True,
            "budget_threshold": 5.0,  # $5 daily budget
        }
        
        # Enable auto-instrumentation - this enhances ALL OpenLLMetry operations
        auto_instrument(**governance_config)
        
        print("üõ°Ô∏è Auto-instrumentation configured:")
        print(f"   ‚Ä¢ Team attribution: {governance_config['team']}")
        print(f"   ‚Ä¢ Project tracking: {governance_config['project']}")
        print(f"   ‚Ä¢ Environment: {governance_config['environment']}")
        print(f"   ‚Ä¢ Budget monitoring: ${governance_config['budget_threshold']}/day")
        print("   ‚Ä¢ Cost alerts: Enabled")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Failed to import GenOps auto-instrumentation: {e}")
        print("üí° Fix: Run 'pip install genops[traceloop]'")
        return False
    except Exception as e:
        print(f"‚ùå Auto-instrumentation setup failed: {e}")
        print("üîß Setup Troubleshooting:")
        print("   ‚Ä¢ Verify OpenLLMetry installation: pip list | grep openllmetry")
        print("   ‚Ä¢ Check GenOps installation: pip install genops[traceloop]")
        print("   ‚Ä¢ Restart Python interpreter after installation")
        if "import" in str(e).lower():
            print("   üí° Import Error: Missing dependencies - run 'pip install genops[traceloop]'")
        elif "version" in str(e).lower():
            print("   üí° Version Conflict: Update packages - run 'pip install --upgrade genops[traceloop]'")
        return False


def existing_openllmetry_code():
    """
    Simulate existing OpenLLMetry application code.
    
    This represents code that already exists and uses OpenLLMetry patterns.
    With GenOps auto-instrumentation, this code gets enhanced automatically
    without any modifications.
    """
    print("\nüìù Running Existing OpenLLMetry Application Code")
    print("-" * 45)
    print("‚ÑπÔ∏è  Note: This code remains unchanged - GenOps enhancement is automatic")
    
    try:
        # Standard OpenLLMetry imports and setup
        import openai
        from openllmetry.instrumentation.openai import OpenAIInstrumentor
        
        # Initialize OpenLLMetry instrumentation (standard pattern)
        OpenAIInstrumentor().instrument()
        
        client = openai.OpenAI()
        print("‚úÖ Standard OpenLLMetry instrumentation initialized")
        
    except ImportError as e:
        print(f"‚ùå OpenLLMetry dependencies missing: {e}")
        print("üí° Fix: Run 'pip install openllmetry'")
        return False
    
    # Example 1: Standard chat completion (unchanged existing code)
    print("\n1Ô∏è‚É£ Standard Chat Completion (Existing Code)")
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Explain auto-instrumentation benefits."}
            ],
            max_tokens=100
        )
        
        content = response.choices[0].message.content
        print(f"‚úÖ Response: {content[:80]}...")
        print("üõ°Ô∏è GenOps governance automatically applied:")
        print("   ‚Ä¢ Cost calculated and attributed to team")
        print("   ‚Ä¢ Team and project context added to trace")
        print("   ‚Ä¢ Budget monitoring active")
        
    except Exception as e:
        print(f"‚ùå Chat completion failed: {e}")
        return False
    
    # Example 2: Multiple operations (unchanged existing code)
    print("\n2Ô∏è‚É£ Batch Operations (Existing Code)")
    try:
        prompts = [
            "What is machine learning?",
            "Explain neural networks briefly.",
            "What are transformers in AI?"
        ]
        
        total_responses = []
        for i, prompt in enumerate(prompts):
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50
            )
            
            content = response.choices[0].message.content
            total_responses.append(content)
            print(f"   ‚úÖ Batch item {i+1}: Generated response")
        
        print(f"‚úÖ Processed {len(total_responses)} prompts")
        print("üõ°Ô∏è GenOps automatically provided:")
        print("   ‚Ä¢ Individual cost tracking for each operation")
        print("   ‚Ä¢ Batch-level cost aggregation")
        print("   ‚Ä¢ Team attribution for entire batch")
        print("   ‚Ä¢ Budget compliance checking")
        
    except Exception as e:
        print(f"‚ùå Batch operations failed: {e}")
        return False
    
    # Example 3: Streaming (unchanged existing code)
    print("\n3Ô∏è‚É£ Streaming Response (Existing Code)")
    try:
        stream = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Count from 1 to 5"}],
            max_tokens=50,
            stream=True
        )
        
        collected_content = []
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content_piece = chunk.choices[0].delta.content
                collected_content.append(content_piece)
        
        full_response = ''.join(collected_content)
        print(f"‚úÖ Streaming response: {full_response.strip()}")
        print("üõ°Ô∏è GenOps streaming enhancements:")
        print("   ‚Ä¢ Real-time cost calculation during streaming")
        print("   ‚Ä¢ Stream-level governance tracking")
        print("   ‚Ä¢ Automatic completion cost attribution")
        
    except Exception as e:
        print(f"‚ùå Streaming failed: {e}")
        return False
    
    return True


def demonstrate_governance_transparency():
    """Show how auto-instrumentation provides governance transparency."""
    print("\nüëÄ Governance Transparency Demo")
    print("-" * 35)
    
    try:
        from genops.providers.traceloop import get_current_governance_context
        
        # Get current governance context (added by auto-instrumentation)
        context = get_current_governance_context()
        
        print("‚úÖ Current governance context:")
        print(f"   ‚Ä¢ Team: {context.get('team', 'N/A')}")
        print(f"   ‚Ä¢ Project: {context.get('project', 'N/A')}")
        print(f"   ‚Ä¢ Environment: {context.get('environment', 'N/A')}")
        print(f"   ‚Ä¢ Cost center: {context.get('cost_center', 'N/A')}")
        
        # Show budget status
        from genops.providers.traceloop import get_budget_status
        budget_status = get_budget_status()
        
        print("\nüí∞ Budget monitoring status:")
        print(f"   ‚Ä¢ Daily budget: ${budget_status.get('daily_limit', 'N/A')}")
        print(f"   ‚Ä¢ Current usage: ${budget_status.get('current_usage', 0.00):.4f}")
        print(f"   ‚Ä¢ Remaining: ${budget_status.get('remaining', 'N/A')}")
        
        # Show recent operations summary
        from genops.providers.traceloop import get_recent_operations_summary
        summary = get_recent_operations_summary(limit=5)
        
        print("\nüìä Recent operations summary:")
        for i, op in enumerate(summary.get('operations', [])):
            print(f"   {i+1}. {op.get('operation_type', 'unknown')}: ${op.get('cost', 0.00):.6f}")
        
        total_cost = summary.get('total_cost', 0.0)
        print(f"   Total recent cost: ${total_cost:.6f}")
        
    except Exception as e:
        print(f"‚ùå Governance transparency demo failed: {e}")
        return False
    
    return True


def show_migration_benefits():
    """Show benefits of migrating to GenOps-enhanced OpenLLMetry."""
    print("\nüîÑ Migration Benefits")
    print("-" * 20)
    
    print("‚úÖ Zero Code Changes Required:")
    print("   ‚Ä¢ Keep your existing OpenLLMetry code")
    print("   ‚Ä¢ Add one line: auto_instrument(team='your-team', project='your-project')")
    print("   ‚Ä¢ All existing operations get enhanced automatically")
    
    print("\nüí∞ Immediate Cost Intelligence:")
    print("   ‚Ä¢ Automatic cost calculation for all operations")
    print("   ‚Ä¢ Team and project cost attribution")
    print("   ‚Ä¢ Real-time budget monitoring and alerts")
    
    print("\nüõ°Ô∏è Governance Without Complexity:")
    print("   ‚Ä¢ Policy enforcement integrated into existing workflows")
    print("   ‚Ä¢ Compliance tracking for audit requirements")
    print("   ‚Ä¢ No changes to deployment or infrastructure")
    
    print("\nüîç Enhanced Observability:")
    print("   ‚Ä¢ All existing OpenTelemetry backends work unchanged")
    print("   ‚Ä¢ Enhanced traces with business context")
    print("   ‚Ä¢ Governance attributes in every span")
    
    print("\nüè¢ Enterprise Ready:")
    print("   ‚Ä¢ Scales with your existing OpenLLMetry infrastructure")
    print("   ‚Ä¢ Optional Traceloop platform integration")
    print("   ‚Ä¢ Professional support and enterprise features available")


def demonstrate_compatibility():
    """Demonstrate compatibility with existing OpenLLMetry patterns."""
    print("\nüîó Compatibility Demonstration")
    print("-" * 30)
    
    try:
        # Show that existing OpenLLMetry patterns still work
        from openllmetry import tracer
        from genops.providers.traceloop import is_enhanced_tracer
        
        # Check if tracer is enhanced with GenOps
        enhanced = is_enhanced_tracer(tracer)
        print(f"‚úÖ OpenLLMetry tracer enhanced: {enhanced}")
        
        # Show that manual spans still work with enhancement
        with tracer.start_span("manual_span_example") as span:
            span.set_attribute("user.action", "manual_span_creation")
            span.set_attribute("custom.attribute", "works_as_expected")
            
            # GenOps automatically adds governance attributes
            print("‚úÖ Manual span created with automatic GenOps enhancement")
            print("   ‚Ä¢ Original OpenLLMetry attributes preserved")
            print("   ‚Ä¢ GenOps governance attributes added automatically")
            print("   ‚Ä¢ Cost tracking enabled for manual spans")
        
        # Show decorator compatibility
        from openllmetry.decorators import workflow
        
        @workflow(name="existing_workflow")
        def existing_decorated_function():
            """Existing function with OpenLLMetry decorator."""
            import openai
            client = openai.OpenAI()
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": "Test compatibility"}],
                max_tokens=20
            )
            return response.choices[0].message.content
        
        # Execute decorated function - gets both OpenLLMetry and GenOps enhancement
        result = existing_decorated_function()
        print("‚úÖ Existing @workflow decorator enhanced automatically")
        print("   ‚Ä¢ OpenLLMetry workflow tracking preserved")
        print("   ‚Ä¢ GenOps governance added seamlessly")
        print(f"   ‚Ä¢ Result: {result[:50]}...")
        
    except Exception as e:
        print(f"‚ùå Compatibility demo failed: {e}")
        return False
    
    return True


async def main():
    """Main execution function."""
    print("‚ö° Auto-Instrumentation OpenLLMetry + GenOps Demo")
    print(f"üïí Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Check prerequisites
    if not os.getenv('OPENAI_API_KEY'):
        print("‚ùå OPENAI_API_KEY not found")
        print("üí° Set your OpenAI API key: export OPENAI_API_KEY='your-key'")
        return False
    
    # Run demo steps
    success = True
    
    # Set up auto-instrumentation
    if not setup_auto_instrumentation():
        success = False
    
    # Run existing code (unchanged)
    if success and not existing_openllmetry_code():
        success = False
    
    # Show governance transparency
    if success and not demonstrate_governance_transparency():
        success = False
    
    # Show compatibility
    if success and not demonstrate_compatibility():
        success = False
    
    # Show migration benefits
    if success:
        show_migration_benefits()
    
    if success:
        print("\n" + "‚ö°" * 55)
        print("üéâ Auto-Instrumentation Demo Complete!")
        
        print("\nüöÄ What You've Accomplished:")
        print("   ‚úÖ Zero-code enhancement of existing OpenLLMetry applications")
        print("   ‚úÖ Automatic governance for all LLM operations")
        print("   ‚úÖ Cost attribution and budget monitoring")
        print("   ‚úÖ 100% compatibility with existing code")
        
        print("\nüí° Implementation in Your App:")
        print("   1. Add to your startup code:")
        print("      ```python")
        print("      from genops.providers.traceloop import auto_instrument")
        print("      auto_instrument(team='your-team', project='your-project')")
        print("      ```")
        print("   2. That's it! All existing OpenLLMetry code is enhanced")
        
        print("\nüìä Immediate Benefits:")
        print("   ‚Ä¢ üîç Enhanced observability with governance context")
        print("   ‚Ä¢ üí∞ Automatic cost calculation and attribution")
        print("   ‚Ä¢ üõ°Ô∏è Policy enforcement and compliance tracking")
        print("   ‚Ä¢ üìà Budget monitoring and cost optimization")
        
        print("\nüìö Next Steps:")
        print("   ‚Ä¢ Customize governance policies for your organization")
        print("   ‚Ä¢ Set up budget alerts and approval workflows")
        print("   ‚Ä¢ Explore Traceloop platform for advanced insights")
        print("   ‚Ä¢ Integrate with your existing observability stack")
        
        print("‚ö°" * 55)
    else:
        print("\n‚ùå Demo encountered errors. Please check the output above.")
    
    return success


if __name__ == "__main__":
    asyncio.run(main())