#!/usr/bin/env python3
"""
Traceloop Commercial Platform + GenOps Integration Example

This example demonstrates how to use Traceloop's commercial platform features
enhanced with GenOps governance, including advanced insights, team collaboration,
model experimentation, and enterprise-grade observability.

The Traceloop platform builds on the OpenLLMetry foundation to provide:
- Advanced insights and analytics
- Model experimentation and A/B testing  
- Team collaboration features
- Enterprise observability dashboards
- Cost optimization recommendations

Usage:
    python traceloop_platform.py

Prerequisites:
    pip install genops[traceloop] traceloop-sdk
    export OPENAI_API_KEY="your-openai-api-key"
    export TRACELOOP_API_KEY="your-traceloop-api-key"  # From app.traceloop.com
    
    # Optional: Custom Traceloop instance
    export TRACELOOP_BASE_URL="https://app.traceloop.com"  # Default
"""

import os
import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import json


def setup_traceloop_platform_integration():
    """
    Set up Traceloop commercial platform integration with GenOps governance.
    
    This demonstrates how to configure the commercial platform features
    while maintaining the OpenLLMetry foundation and adding GenOps governance.
    """
    print("ğŸ¢ Traceloop Commercial Platform + GenOps Integration")
    print("=" * 55)
    
    # Check prerequisites
    api_key = os.getenv('TRACELOOP_API_KEY')
    if not api_key:
        print("âŒ TRACELOOP_API_KEY not found")
        print("ğŸ’¡ Get your API key from: https://app.traceloop.com")
        print("   Set it with: export TRACELOOP_API_KEY='your-api-key'")
        return False
        
    try:
        # Import GenOps Traceloop adapter with platform integration
        from genops.providers.traceloop import instrument_traceloop
        print("âœ… GenOps Traceloop adapter loaded")
        
        # Import Traceloop SDK for commercial platform features
        from traceloop.sdk import Traceloop
        from traceloop.sdk.decorators import workflow
        print("âœ… Traceloop SDK loaded for commercial platform")
        
        # Initialize with commercial platform enabled
        adapter = instrument_traceloop(
            team="commercial-team",
            project="platform-demo",
            environment="production",
            
            # Enable commercial platform features
            enable_traceloop_platform=True,
            traceloop_api_key=api_key,
            
            # Enhanced governance for commercial usage
            enable_governance=True,
            daily_budget_limit=10.0,  # $10 daily budget
            enable_cost_alerts=True,
            cost_alert_threshold=2.0,  # Alert above $2
            
            # Commercial platform specific settings
            enable_advanced_analytics=True,
            enable_team_collaboration=True,
            enable_model_experimentation=True
        )
        
        print("ğŸ¢ Commercial platform features enabled:")
        print("   â€¢ Advanced insights and analytics")
        print("   â€¢ Team collaboration and sharing")
        print("   â€¢ Model experimentation and A/B testing")
        print("   â€¢ Enterprise observability dashboards")
        print("   â€¢ Cost optimization recommendations")
        
        return adapter
        
    except ImportError as e:
        print(f"âŒ Failed to import required dependencies: {e}")
        print("ğŸ’¡ Install with: pip install genops[traceloop] traceloop-sdk")
        return None
    except Exception as e:
        print(f"âŒ Platform setup failed: {e}")
        return None


def demonstrate_advanced_insights(adapter):
    """Demonstrate advanced insights and analytics from Traceloop platform."""
    print("\nğŸ“Š Advanced Insights and Analytics")
    print("-" * 35)
    
    try:
        import openai
        client = openai.OpenAI()
        
        # Example 1: Multi-model comparison with insights
        models_to_test = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"]
        prompt = "Explain the benefits of LLM observability in one paragraph."
        
        results = {}
        for model in models_to_test:
            with adapter.track_operation(
                operation_type="model_comparison",
                operation_name=f"insights_test_{model}",
                tags={"model": model, "experiment": "model_comparison", "team": "research"}
            ) as span:
                
                start_time = time.time()
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=150,
                    temperature=0.7
                )
                
                # Record detailed metrics for platform insights
                metrics = span.get_metrics()
                duration = time.time() - start_time
                
                results[model] = {
                    "response": response.choices[0].message.content,
                    "cost": metrics.get('estimated_cost', 0),
                    "tokens": metrics.get('total_tokens', 0),
                    "duration": duration,
                    "model": model
                }
                
                # Platform-specific metadata for advanced insights
                span.add_attributes({
                    "experiment.name": "model_comparison",
                    "experiment.variant": model,
                    "quality.response_length": len(response.choices[0].message.content),
                    "quality.coherence_score": 0.85,  # Would be calculated
                    "business.use_case": "customer_support",
                    "business.priority": "high"
                })
                
                print(f"   âœ… {model}: ${metrics.get('estimated_cost', 0):.6f} ({metrics.get('total_tokens', 0)} tokens)")
        
        # Platform insights summary (would be enhanced by Traceloop platform)
        print("\nğŸ“ˆ Platform Insights Generated:")
        print("   â€¢ Model performance comparison across cost/quality metrics")
        print("   â€¢ Automatic quality scoring and coherence analysis")
        print("   â€¢ Business context attribution for ROI analysis")
        print("   â€¢ Team-based cost optimization recommendations")
        
        # Best model recommendation based on cost/performance
        best_model = min(results.keys(), key=lambda k: results[k]['cost'])
        print(f"   ğŸ’¡ Recommended model for this use case: {best_model}")
        
        return results
        
    except Exception as e:
        print(f"âŒ Advanced insights demo failed: {e}")
        return None


def demonstrate_team_collaboration(adapter):
    """Demonstrate team collaboration features."""
    print("\nğŸ‘¥ Team Collaboration Features")
    print("-" * 30)
    
    try:
        import openai
        client = openai.OpenAI()
        
        # Simulate team-based workflow with collaboration features
        teams = [
            {"name": "frontend-team", "project": "chat-interface"},
            {"name": "backend-team", "project": "api-services"},
            {"name": "data-team", "project": "analytics-engine"}
        ]
        
        shared_metrics = {}
        
        for team_info in teams:
            # Create team-specific adapter instance
            team_adapter = instrument_traceloop(
                team=team_info["name"],
                project=team_info["project"],
                environment="production",
                enable_traceloop_platform=True,
                enable_team_collaboration=True
            )
            
            with team_adapter.track_operation(
                operation_type="team_collaboration",
                operation_name=f"shared_workflow_{team_info['name']}",
                tags={
                    "team": team_info["name"],
                    "shared_experiment": "cross_team_optimization",
                    "collaboration_id": "shared_cost_optimization"
                }
            ) as span:
                
                # Simulate team-specific LLM operations
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{
                        "role": "user", 
                        "content": f"Generate a {team_info['name']} specific optimization tip"
                    }],
                    max_tokens=80
                )
                
                metrics = span.get_metrics()
                shared_metrics[team_info["name"]] = metrics
                
                # Platform collaboration metadata
                span.add_attributes({
                    "collaboration.experiment_id": "shared_cost_optimization",
                    "collaboration.shared_budget": True,
                    "collaboration.cross_team_visibility": True,
                    "team.department": team_info["name"].split("-")[0],
                    "team.project_phase": "optimization"
                })
                
                print(f"   âœ… {team_info['name']}: ${metrics.get('estimated_cost', 0):.6f}")
        
        print("\nğŸ¤ Collaboration Features Enabled:")
        print("   â€¢ Cross-team cost visibility and attribution")
        print("   â€¢ Shared experiment tracking and results")
        print("   â€¢ Team-based budget allocation and monitoring")
        print("   â€¢ Collaborative optimization recommendations")
        print("   â€¢ Enterprise audit trails and compliance reporting")
        
        # Calculate shared metrics
        total_cost = sum(metrics.get('estimated_cost', 0) for metrics in shared_metrics.values())
        print(f"   ğŸ’° Total cross-team cost: ${total_cost:.6f}")
        
        return shared_metrics
        
    except Exception as e:
        print(f"âŒ Team collaboration demo failed: {e}")
        return None


def demonstrate_model_experimentation(adapter):
    """Demonstrate model experimentation and A/B testing features."""
    print("\nğŸ§ª Model Experimentation & A/B Testing")
    print("-" * 40)
    
    try:
        import openai
        client = openai.OpenAI()
        
        # Set up A/B test experiment
        experiment_config = {
            "experiment_name": "prompt_optimization_v2",
            "variants": [
                {
                    "name": "control",
                    "prompt": "Summarize the following text:",
                    "temperature": 0.7,
                    "model": "gpt-3.5-turbo"
                },
                {
                    "name": "variant_a", 
                    "prompt": "Please provide a concise summary of this content:",
                    "temperature": 0.5,
                    "model": "gpt-3.5-turbo"
                },
                {
                    "name": "variant_b",
                    "prompt": "Summarize the following text:",
                    "temperature": 0.7,
                    "model": "gpt-4"
                }
            ],
            "success_metrics": ["cost", "response_quality", "user_satisfaction"]
        }
        
        test_content = "LLM observability is crucial for production applications because it provides visibility into model performance, cost attribution, and quality metrics that enable teams to optimize their AI operations effectively."
        
        experiment_results = {}
        
        for variant in experiment_config["variants"]:
            variant_name = variant["name"]
            
            with adapter.track_operation(
                operation_type="ab_test_experiment",
                operation_name=f"experiment_{variant_name}",
                tags={
                    "experiment_name": experiment_config["experiment_name"],
                    "variant": variant_name,
                    "hypothesis": "improved_prompt_reduces_cost"
                }
            ) as span:
                
                # Execute variant
                full_prompt = f"{variant['prompt']}\n\n{test_content}"
                
                response = client.chat.completions.create(
                    model=variant["model"],
                    messages=[{"role": "user", "content": full_prompt}],
                    max_tokens=100,
                    temperature=variant["temperature"]
                )
                
                # Collect experiment metrics
                metrics = span.get_metrics()
                response_text = response.choices[0].message.content
                
                # Platform experimentation metadata
                span.add_attributes({
                    "experiment.name": experiment_config["experiment_name"],
                    "experiment.variant": variant_name,
                    "experiment.hypothesis": "improved_prompt_reduces_cost",
                    "experiment.control_group": variant_name == "control",
                    "quality.response_length": len(response_text),
                    "quality.relevance_score": 0.92,  # Would be calculated
                    "performance.model": variant["model"],
                    "performance.temperature": variant["temperature"]
                })
                
                experiment_results[variant_name] = {
                    "cost": metrics.get('estimated_cost', 0),
                    "tokens": metrics.get('total_tokens', 0),
                    "response_length": len(response_text),
                    "model": variant["model"],
                    "temperature": variant["temperature"]
                }
                
                print(f"   âœ… {variant_name}: ${metrics.get('estimated_cost', 0):.6f} ({len(response_text)} chars)")
        
        # Experiment analysis (enhanced by Traceloop platform)
        print("\nğŸ“Š Experiment Analysis:")
        control_cost = experiment_results["control"]["cost"]
        
        for variant_name, results in experiment_results.items():
            if variant_name != "control":
                cost_diff = ((results["cost"] - control_cost) / control_cost) * 100
                print(f"   â€¢ {variant_name} vs control: {cost_diff:+.1f}% cost difference")
        
        print("\nğŸ”¬ Platform Experimentation Features:")
        print("   â€¢ Statistical significance testing")
        print("   â€¢ Automatic winner determination")  
        print("   â€¢ Confidence intervals and p-values")
        print("   â€¢ Multi-metric optimization (cost + quality)")
        print("   â€¢ Experiment lifecycle management")
        
        return experiment_results
        
    except Exception as e:
        print(f"âŒ Model experimentation demo failed: {e}")
        return None


def demonstrate_enterprise_observability(adapter):
    """Demonstrate enterprise observability features."""
    print("\nğŸ¢ Enterprise Observability Dashboard")
    print("-" * 35)
    
    try:
        # Simulate enterprise dashboard data collection
        dashboard_metrics = {
            "operational_health": {
                "total_requests": 1247,
                "success_rate": 99.2,
                "avg_latency_ms": 245,
                "error_rate": 0.8
            },
            "cost_intelligence": {
                "daily_spend": 24.67,
                "monthly_projection": 740.10,
                "budget_utilization": 0.67,
                "cost_per_request": 0.0198
            },
            "team_attribution": {
                "frontend-team": 8.45,
                "backend-team": 12.22,
                "data-team": 4.00
            },
            "governance_compliance": {
                "policy_violations": 0,
                "budget_alerts": 2,
                "cost_approvals_pending": 1,
                "compliance_score": 98.5
            }
        }
        
        # Display enterprise dashboard summary
        print("ğŸ“Š Real-time Dashboard Metrics:")
        print(f"   â€¢ Daily spend: ${dashboard_metrics['cost_intelligence']['daily_spend']}")
        print(f"   â€¢ Success rate: {dashboard_metrics['operational_health']['success_rate']}%")
        print(f"   â€¢ Compliance score: {dashboard_metrics['governance_compliance']['compliance_score']}%")
        print(f"   â€¢ Policy violations: {dashboard_metrics['governance_compliance']['policy_violations']}")
        
        print("\nğŸ¯ Cost Attribution by Team:")
        for team, cost in dashboard_metrics["team_attribution"].items():
            percentage = (cost / dashboard_metrics['cost_intelligence']['daily_spend']) * 100
            print(f"   â€¢ {team}: ${cost:.2f} ({percentage:.1f}%)")
        
        print("\nğŸ›¡ï¸ Governance & Compliance:")
        governance = dashboard_metrics["governance_compliance"]
        print(f"   â€¢ Policy violations: {governance['policy_violations']} (âœ… Compliant)")
        print(f"   â€¢ Budget alerts: {governance['budget_alerts']} (âš ï¸ Monitor)")
        print(f"   â€¢ Pending approvals: {governance['cost_approvals_pending']}")
        
        print("\nğŸ¢ Enterprise Features Available:")
        print("   â€¢ Real-time operational dashboards")
        print("   â€¢ Advanced cost intelligence and forecasting")
        print("   â€¢ Multi-team governance and policy enforcement")
        print("   â€¢ Compliance reporting and audit trails")
        print("   â€¢ Executive summaries and ROI analysis")
        print("   â€¢ Integration with enterprise observability stacks")
        
        # Generate platform recommendations
        print("\nğŸ’¡ Platform Recommendations:")
        if governance['budget_alerts'] > 0:
            print("   âš ï¸ Consider optimizing high-cost operations")
        if dashboard_metrics['cost_intelligence']['budget_utilization'] > 0.8:
            print("   ğŸ“ˆ Budget utilization high - consider increasing limit")
        print("   ğŸ¯ Focus optimization on backend-team (highest spend)")
        
        return dashboard_metrics
        
    except Exception as e:
        print(f"âŒ Enterprise observability demo failed: {e}")
        return None


async def main():
    """Main execution function."""
    print("ğŸ¢ Traceloop Commercial Platform + GenOps Demo")
    print(f"ğŸ•’ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Check prerequisites
    if not os.getenv('OPENAI_API_KEY'):
        print("âŒ OPENAI_API_KEY not found")
        print("ğŸ’¡ Set your OpenAI API key: export OPENAI_API_KEY='your-key'")
        return False
    
    if not os.getenv('TRACELOOP_API_KEY'):
        print("âŒ TRACELOOP_API_KEY not found")
        print("ğŸ’¡ Sign up at https://app.traceloop.com and get your API key")
        return False
    
    # Set up platform integration
    adapter = setup_traceloop_platform_integration()
    if not adapter:
        return False
    
    # Run platform demos
    success = True
    
    # Advanced insights
    insights_results = demonstrate_advanced_insights(adapter)
    if not insights_results:
        success = False
    
    # Team collaboration
    if success:
        collaboration_results = demonstrate_team_collaboration(adapter)
        if not collaboration_results:
            success = False
    
    # Model experimentation
    if success:
        experiment_results = demonstrate_model_experimentation(adapter)
        if not experiment_results:
            success = False
    
    # Enterprise observability
    if success:
        dashboard_results = demonstrate_enterprise_observability(adapter)
        if not dashboard_results:
            success = False
    
    if success:
        print("\n" + "ğŸ¢" * 60)
        print("ğŸ‰ Traceloop Commercial Platform + GenOps Demo Complete!")
        
        print("\nğŸš€ What You've Accomplished:")
        print("   âœ… Integrated commercial platform with GenOps governance")
        print("   âœ… Advanced insights and analytics with cost intelligence")
        print("   âœ… Team collaboration with shared experiment tracking")
        print("   âœ… A/B testing and model experimentation capabilities")
        print("   âœ… Enterprise observability with compliance monitoring")
        
        print("\nğŸ¢ Commercial Platform Benefits:")
        print("   â€¢ ğŸ“Š Advanced analytics beyond basic OpenLLMetry")
        print("   â€¢ ğŸ¤ Team collaboration and shared experiment management")
        print("   â€¢ ğŸ§ª Statistical A/B testing with automated winner selection")
        print("   â€¢ ğŸ—ï¸ Enterprise dashboards with executive reporting")
        print("   â€¢ ğŸ›¡ï¸ Enhanced compliance and audit capabilities")
        print("   â€¢ ğŸ’¼ Professional support and custom integrations")
        
        print("\nğŸ’¡ Upgrade Path from Open Source:")
        print("   1. Keep your existing OpenLLMetry foundation")
        print("   2. Add Traceloop API key for commercial features")
        print("   3. Enhanced insights and team collaboration automatically enabled")
        print("   4. Access to advanced experimentation and enterprise dashboards")
        
        print("\nğŸ“š Next Steps:")
        print("   â€¢ Explore production deployment with production_patterns.py")
        print("   â€¢ Set up team-specific dashboards and alerts")
        print("   â€¢ Configure advanced governance policies for your organization")
        print("   â€¢ Integrate with your existing enterprise observability stack")
        
        print("ğŸ¢" * 60)
    else:
        print("\nâŒ Demo encountered errors. Please check the output above.")
    
    return success


if __name__ == "__main__":
    asyncio.run(main())